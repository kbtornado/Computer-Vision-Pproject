{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a4ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import accuracy_score,f1_score, classification_report, confusion_matrix , accuracy_score, precision_score, recall_score, f1_score, roc_curve ,roc_auc_score,ConfusionMatrixDisplay\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf95c6",
   "metadata": {},
   "source": [
    "Path Definition: Paths for four folders are defined, each containing images of different categories of brain scans: glioma tumor, meningioma tumor, no tumor, and pituitary tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of dataset\n",
    "folder1=\"glioma_tumor\"\n",
    "folder2=\"meningioma_tumor\"\n",
    "folder3=\"no_tumor\"\n",
    "folder4=\"pituitary_tumor\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd20b7c",
   "metadata": {},
   "source": [
    "\n",
    "#### Feature Extraction Using HOG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac09a94",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction Using HOG (Histogram of Oriented Gradients): The core of this script lies in extracting features from the images using the Histogram of Oriented Gradients (HOG) method. This process is repeated for each folder (each tumor type). The steps for feature extraction include:\n",
    "\n",
    "Reading each image from the dataset.\n",
    "Resizing images to a standard size (200x200 pixels) to ensure uniformity.\n",
    "Converting images to grayscale, which is a common preprocessing step in image processing to reduce complexity while retaining essential features.\n",
    "Applying a median filter with a 3x3 kernel to reduce noise in the images, which helps in highlighting important features while suppressing irrelevant details.\n",
    "Extracting HOG features from the preprocessed images. HOG is a feature descriptor that is particularly effective for object detection in computer vision. It works by counting occurrences of gradient orientation in localized portions of an image.\n",
    "Data Structuring and Storage: After extracting the HOG descriptors, they are stored in a pandas DataFrame, with each row representing the HOG feature vector of an image. A 'Class' column is added to this DataFrame to label the data according to the type of tumor (or absence thereof) it represents. This labeling is crucial for supervised learning tasks.\n",
    "\n",
    "Saving the Data: The feature vectors along with their labels are saved into .npy files for each category of brain scans. This file format is efficient for storing and accessing large arrays, making it suitable for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor shape  0  :  (15000, 1)\n",
      "descriptor shape  1  :  (15000, 1)\n",
      "descriptor shape  2  :  (15000, 1)\n",
      "descriptor shape  3  :  (15000, 1)\n",
      "descriptor shape  4  :  (15000, 1)\n",
      "descriptor shape  5  :  (15000, 1)\n",
      "descriptor shape  6  :  (15000, 1)\n",
      "descriptor shape  7  :  (15000, 1)\n",
      "descriptor shape  8  :  (15000, 1)\n",
      "descriptor shape  9  :  (15000, 1)\n",
      "descriptor shape  10  :  (15000, 1)\n",
      "descriptor shape  11  :  (15000, 1)\n",
      "descriptor shape  12  :  (15000, 1)\n",
      "descriptor shape  13  :  (15000, 1)\n",
      "descriptor shape  14  :  (15000, 1)\n",
      "descriptor shape  15  :  (15000, 1)\n",
      "descriptor shape  16  :  (15000, 1)\n",
      "descriptor shape  17  :  (15000, 1)\n",
      "descriptor shape  18  :  (15000, 1)\n",
      "descriptor shape  19  :  (15000, 1)\n",
      "descriptor shape  20  :  (15000, 1)\n",
      "descriptor shape  21  :  (15000, 1)\n",
      "descriptor shape  22  :  (15000, 1)\n",
      "descriptor shape  23  :  (15000, 1)\n",
      "descriptor shape  24  :  (15000, 1)\n",
      "descriptor shape  25  :  (15000, 1)\n",
      "descriptor shape  26  :  (15000, 1)\n",
      "descriptor shape  27  :  (15000, 1)\n",
      "descriptor shape  28  :  (15000, 1)\n",
      "descriptor shape  29  :  (15000, 1)\n",
      "descriptor shape  30  :  (15000, 1)\n",
      "descriptor shape  31  :  (15000, 1)\n",
      "descriptor shape  32  :  (15000, 1)\n",
      "descriptor shape  33  :  (15000, 1)\n",
      "descriptor shape  34  :  (15000, 1)\n",
      "descriptor shape  35  :  (15000, 1)\n",
      "descriptor shape  36  :  (15000, 1)\n",
      "descriptor shape  37  :  (15000, 1)\n",
      "descriptor shape  38  :  (15000, 1)\n",
      "descriptor shape  39  :  (15000, 1)\n",
      "descriptor shape  40  :  (15000, 1)\n",
      "descriptor shape  41  :  (15000, 1)\n",
      "descriptor shape  42  :  (15000, 1)\n",
      "descriptor shape  43  :  (15000, 1)\n",
      "descriptor shape  44  :  (15000, 1)\n",
      "descriptor shape  45  :  (15000, 1)\n",
      "descriptor shape  46  :  (15000, 1)\n",
      "descriptor shape  47  :  (15000, 1)\n",
      "descriptor shape  48  :  (15000, 1)\n",
      "descriptor shape  49  :  (15000, 1)\n",
      "descriptor shape  50  :  (15000, 1)\n",
      "descriptor shape  51  :  (15000, 1)\n",
      "descriptor shape  52  :  (15000, 1)\n",
      "descriptor shape  53  :  (15000, 1)\n",
      "descriptor shape  54  :  (15000, 1)\n",
      "descriptor shape  55  :  (15000, 1)\n",
      "descriptor shape  56  :  (15000, 1)\n",
      "descriptor shape  57  :  (15000, 1)\n",
      "descriptor shape  58  :  (15000, 1)\n",
      "descriptor shape  59  :  (15000, 1)\n",
      "descriptor shape  60  :  (15000, 1)\n",
      "descriptor shape  61  :  (15000, 1)\n",
      "descriptor shape  62  :  (15000, 1)\n",
      "descriptor shape  63  :  (15000, 1)\n",
      "descriptor shape  64  :  (15000, 1)\n",
      "descriptor shape  65  :  (15000, 1)\n",
      "descriptor shape  66  :  (15000, 1)\n",
      "descriptor shape  67  :  (15000, 1)\n",
      "descriptor shape  68  :  (15000, 1)\n",
      "descriptor shape  69  :  (15000, 1)\n",
      "descriptor shape  70  :  (15000, 1)\n",
      "descriptor shape  71  :  (15000, 1)\n",
      "descriptor shape  72  :  (15000, 1)\n",
      "descriptor shape  73  :  (15000, 1)\n",
      "descriptor shape  74  :  (15000, 1)\n",
      "descriptor shape  75  :  (15000, 1)\n",
      "descriptor shape  76  :  (15000, 1)\n",
      "descriptor shape  77  :  (15000, 1)\n",
      "descriptor shape  78  :  (15000, 1)\n",
      "descriptor shape  79  :  (15000, 1)\n",
      "descriptor shape  80  :  (15000, 1)\n",
      "descriptor shape  81  :  (15000, 1)\n",
      "descriptor shape  82  :  (15000, 1)\n",
      "descriptor shape  83  :  (15000, 1)\n",
      "descriptor shape  84  :  (15000, 1)\n",
      "descriptor shape  85  :  (15000, 1)\n",
      "descriptor shape  86  :  (15000, 1)\n",
      "descriptor shape  87  :  (15000, 1)\n",
      "descriptor shape  88  :  (15000, 1)\n",
      "descriptor shape  89  :  (15000, 1)\n",
      "descriptor shape  90  :  (15000, 1)\n",
      "descriptor shape  91  :  (15000, 1)\n",
      "descriptor shape  92  :  (15000, 1)\n",
      "descriptor shape  93  :  (15000, 1)\n",
      "descriptor shape  94  :  (15000, 1)\n",
      "descriptor shape  95  :  (15000, 1)\n",
      "descriptor shape  96  :  (15000, 1)\n",
      "descriptor shape  97  :  (15000, 1)\n",
      "descriptor shape  98  :  (15000, 1)\n",
      "descriptor shape  99  :  (15000, 1)\n",
      "descriptor shape  100  :  (15000, 1)\n",
      "descriptor shape  101  :  (15000, 1)\n",
      "descriptor shape  102  :  (15000, 1)\n",
      "descriptor shape  103  :  (15000, 1)\n",
      "descriptor shape  104  :  (15000, 1)\n",
      "descriptor shape  105  :  (15000, 1)\n",
      "descriptor shape  106  :  (15000, 1)\n",
      "descriptor shape  107  :  (15000, 1)\n",
      "descriptor shape  108  :  (15000, 1)\n",
      "descriptor shape  109  :  (15000, 1)\n",
      "descriptor shape  110  :  (15000, 1)\n",
      "descriptor shape  111  :  (15000, 1)\n",
      "descriptor shape  112  :  (15000, 1)\n",
      "descriptor shape  113  :  (15000, 1)\n",
      "descriptor shape  114  :  (15000, 1)\n",
      "descriptor shape  115  :  (15000, 1)\n",
      "descriptor shape  116  :  (15000, 1)\n",
      "descriptor shape  117  :  (15000, 1)\n",
      "descriptor shape  118  :  (15000, 1)\n",
      "descriptor shape  119  :  (15000, 1)\n"
     ]
    }
   ],
   "source": [
    "hog_descs = []\n",
    "i = 0\n",
    "for filename in os.listdir(folder1):\n",
    "    # print(os.path.join(folder_path, filename))\n",
    "    img = cv2.imread(os.path.join(folder1, filename))\n",
    "    if img is not None:\n",
    "        \n",
    "        #resize total  image size to 200 x 200\n",
    "        resize=(200,200)\n",
    "        img1=cv2.resize(img,resize)\n",
    "        \n",
    "        # Grayscaling the image dataset\n",
    "        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply median filter with kernel size 3x3\n",
    "        median_img = cv2.medianBlur(gray, 3)\n",
    "    \n",
    "        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "        # Convert the descriptor array into a DataFrame format\n",
    "        hog_descs.append(fd)\n",
    "        df = pd.DataFrame(fd)\n",
    "        # print(df)\n",
    "        print(\"descriptor shape \", i, \" : \", df.shape)\n",
    "        i = i + 1\n",
    "\n",
    "df = pd.DataFrame(hog_descs)\n",
    "i = 0\n",
    "#add row of class\n",
    "df[\"Class\"] = i\n",
    "\n",
    "#Storing previously saved feature descriptor to numpy file .\n",
    "np.save(\"glioma.npy\", df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder 2 (Meningioma Tumor)\n",
    "hog_descs = []\n",
    "i = 0\n",
    "for filename in os.listdir(folder2):\n",
    "    # print(os.path.join(folder_path, filename))\n",
    "    img = cv2.imread(os.path.join(folder2, filename))\n",
    "    if img is not None:\n",
    "        #resize total  image size to 200 x 200\n",
    "        resize=(200,200)\n",
    "        img1=cv2.resize(img,resize)\n",
    "        \n",
    "        # Grayscaling the image dataset\n",
    "        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply median filter with kernel size 3x3\n",
    "        median_img = cv2.medianBlur(gray, 3)\n",
    "\n",
    "        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "        # Convert the descriptor array into a DataFrame format\n",
    "        hog_descs.append(fd)\n",
    "        df = pd.DataFrame(fd)\n",
    "        # print(df)\n",
    "        print(\"descriptor shape \", i, \" : \", df.shape)\n",
    "        i = i + 1\n",
    "\n",
    "df = pd.DataFrame(hog_descs)\n",
    "i = 1\n",
    "#add row class\n",
    "df[\"Class\"] = i\n",
    "\n",
    "#Storing previously saved feature descriptor to numpy file .\n",
    "np.save(\"meningioma.npy\", df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57458a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder 3 (No tumor)\n",
    "hog_descs = []\n",
    "i = 0\n",
    "for filename in os.listdir(folder3):\n",
    "    # print(os.path.join(folder_path, filename))\n",
    "    img = cv2.imread(os.path.join(folder3, filename))\n",
    "    if img is not None:\n",
    "        #resize total  image size to 200 x 200\n",
    "        resize=(200,200)\n",
    "        img1=cv2.resize(img,resize)\n",
    "\n",
    "        # Grayscaling the image dataset\n",
    "        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply median filter with kernel size 3x3\n",
    "        median_img = cv2.medianBlur(gray, 3)\n",
    "\n",
    "        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "        # Convert the descriptor array into a DataFrame format\n",
    "        hog_descs.append(fd)\n",
    "        df = pd.DataFrame(fd)\n",
    "        # print(df)\n",
    "        print(\"descriptor shape \", i, \" : \", df.shape)\n",
    "        i = i + 1\n",
    "\n",
    "df = pd.DataFrame(hog_descs)\n",
    "i = 2\n",
    "#add row class\n",
    "df[\"Class\"] = i\n",
    "\n",
    "#Storing previously saved feature descriptor to numpy file .\n",
    "np.save(\"notumor.npy\", df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder4=\"pituitary_tumor\"\n",
    "#Folder 4 (Pituitary)\n",
    "hog_descs = []\n",
    "i = 0\n",
    "for filename in os.listdir(folder4):\n",
    "    # print(os.path.join(folder_path, filename))\n",
    "    img = cv2.imread(os.path.join(folder4, filename))\n",
    "    if img is not None:\n",
    "        #resize total  image size to 200 x 200\n",
    "        resize=(200,200)\n",
    "        img1=cv2.resize(img,resize)\n",
    "\n",
    "        # Grayscaling the image dataset\n",
    "        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply median filter with kernel size 3x3\n",
    "        median_img = cv2.medianBlur(gray, 3)\n",
    "\n",
    "        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "        # Convert the descriptor array into a DataFrame format\n",
    "        hog_descs.append(fd)\n",
    "        df = pd.DataFrame(fd)\n",
    "        # print(df)\n",
    "        print(\"descriptor shape \", i, \" : \", df.shape)\n",
    "        i = i + 1\n",
    "\n",
    "df = pd.DataFrame(hog_descs)\n",
    "i = 3\n",
    "#add row class\n",
    "df[\"Class\"] = i\n",
    "\n",
    "#Storing previously saved feature descriptor to numpy file .\n",
    "np.save(\"pituitary.npy\", df.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e40e3c",
   "metadata": {},
   "source": [
    "Loading the HOG Features: The code begins by loading the HOG (Histogram of Oriented Gradients) feature arrays for different classes of brain scans from saved .npy files. These classes include 'glioma', 'meningioma', 'no tumor', and 'pituitary tumor'. Each .npy file contains the HOG feature vectors for the respective class of images, where each row corresponds to the HOG descriptor of an individual image, and the last column represents the class label.\n",
    "\n",
    "Concatenating Arrays: The individual arrays for each tumor type are concatenated into a single array using np.concatenate(). This operation combines the feature vectors from all the classes along the first axis (rows), resulting in a single array where each row still corresponds to a feature vector from one of the images, and the rows are ordered by the sequence in which the arrays were concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Hog features of All class Glioma, Meningioma, no tumor\n",
    "glioma_array = np.load('glioma.npy')\n",
    "meningioma_array = np.load('meningioma.npy')\n",
    "notumor_array = np.load('notumor.npy')\n",
    "pitutary_array = np.load('pituitary.npy')\n",
    "\n",
    "concatenated_array = np.concatenate((glioma_array, meningioma_array,notumor_array,pitutary_array), axis=0)\n",
    "\n",
    "# prints the shape of the concatenated array\n",
    "print(concatenated_array.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127f30b",
   "metadata": {},
   "source": [
    "Feature and Label Separation:\n",
    "\n",
    "The features (x) and labels (y) are separated from the concatenated array. The features consist of the HOG descriptors, and the labels represent the class of each image.\n",
    "For x, all columns except the last one are selected (up to 15000 features), which means each row in x contains the HOG feature vector for a corresponding image.\n",
    "For y, the last column is selected, which contains the class labels for each image. These labels are converted to integers using astype(np.int) to ensure they are in a suitable format for classification algorithms.\n",
    "Printing Shapes: Finally, the shapes of x and y are printed to verify their structures. x should have a shape where the number of rows equals the total number of images and the number of columns equals the number of features (up to 15000 in this case). y should be a 1-dimensional array with a length equal to the total number of images, where each element is a class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = concatenated_array[:, :15000]   # selects columns up to 15000 for x\n",
    "y = concatenated_array[:, -1]       # selects the last column for y\n",
    "y = y.astype(np.int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0648c",
   "metadata": {},
   "source": [
    "Shape of the Concatenated Array: The shape of the concatenated array is printed to provide insight into the data structure. The number of rows in this array equals the total number of images across all classes, and the number of columns corresponds to the number of features plus one for the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)  # prints the shape of x\n",
    "print(y.shape)  # prints the shape of y\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6e735",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254ec3b",
   "metadata": {},
   "source": [
    "This code segment employs Principal Component Analysis (PCA), a dimensionality reduction technique, to analyze and visualize the variance explained by the components extracted from the HOG (Histogram of Oriented Gradients) features of brain scans. Here's a breakdown of each step and its purpose:\n",
    "\n",
    "PCA Object Creation: A PCA object is instantiated without specifying the number of components. This means that PCA will consider as many components as there are features in the dataset (x) by default, aiming to capture the entirety of the variance in the data.\n",
    "\n",
    "Fitting the PCA Model: The PCA model is fit to the HOG features stored in x. This process involves calculating the eigenvalues and eigenvectors of the covariance matrix of x, which are used to determine the principal components. These components are the directions in the feature space that maximize the variance of the projected data.\n",
    "\n",
    "Cumulative Explained Variance Ratio: The explained variance ratio of each principal component is calculated. This ratio indicates the proportion of the dataset's total variance that is captured by each principal component. The cumulative sum of these ratios (cumulative_var_ratio) is then computed to understand how much of the total variance is explained by the first n components combined.\n",
    "\n",
    "Explained Variance Ratio Output: The explained variance ratio for each principal component is printed. These values give insight into the importance of each componentâ€”higher values mean a component captures more of the data's variance.\n",
    "\n",
    "Variance Plot: A line plot is created to visualize the cumulative explained variance ratio against the number of components. This plot is crucial for determining the number of components needed to capture a significant portion of the variance in the data. The x-axis represents the number of components, and the y-axis represents the cumulative explained variance ratio.\n",
    "\n",
    "The xlabel 'Number of Components' indicates the principal components on the x-axis.\n",
    "The ylabel 'Cumulative Explained Variance Ratio' indicates the proportion of the dataset's total variance explained by the first n components on the y-axis.\n",
    "The title 'cumulative variance plot.' labels the plot for clarity.\n",
    "The purpose of this analysis is to understand the dimensionality of the HOG feature space and to identify how many principal components are necessary to capture most of the variance in the data. This is often used to reduce the number of features before applying machine learning models, improving computational efficiency and potentially reducing overfitting by eliminating noise and less informative features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f290ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA model to the HOG features\n",
    "pca.fit(x)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the explained variance ratio against number of components\n",
    "plt.plot(cumulative_var_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('cumulative variance plot.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e45fba",
   "metadata": {},
   "source": [
    "PCA Transformation:\n",
    "\n",
    "pca.transform(x): This line applies the PCA transformation to the HOG feature matrix x. PCA is a dimensionality reduction technique that identifies the axes (principal components) along which the variance in the data is maximized. By projecting the data onto these axes, it's possible to reduce the number of dimensions (features) while retaining most of the variance (information) in the data. The result, B, is the transformed dataset where each row represents an image and columns represent the principal components. The number of columns in B is equal to the number of components in the PCA model, which was fit to the data earlier.\n",
    "Converting to DataFrame:\n",
    "\n",
    "B = pd.DataFrame(B): This line converts the transformed feature matrix B into a pandas DataFrame. This is often done for ease of data manipulation and to utilize pandas' functionality for data analysis, as DataFrames are a more flexible and intuitive structure for tabular data.\n",
    "Fitting PCA with a Specified Number of Components:\n",
    "\n",
    "pca = PCA(n_components=100): Here, a new PCA object is created with a specified number of components (n_components=100). This means that when this PCA model is fit and applied to the data, it will reduce the dimensionality of the data to 100 principal components, regardless of the original number of features.\n",
    "pca.fit(x): This line fits the new PCA model to the HOG features (x). The PCA model learns the 100 principal components that capture the most variance in the data. This fitting process involves computing the eigenvectors (principal components) and eigenvalues (explained variance) of the data's covariance matrix.\n",
    "The purpose of these operations is to reduce the dimensionality of the feature set to make subsequent machine learning models more efficient and potentially more effective. By reducing the number of features to a set of principal components that capture the majority of the variance in the data, it's possible to speed up training times, reduce the risk of overfitting, and possibly improve the generalization performance of the models. The choice of 100 components is a balance between retaining enough information (variance) and reducing the feature space to a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B = pca.transform(x)\n",
    "B = pd.DataFrame(B)\n",
    "B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb75591",
   "metadata": {},
   "source": [
    "Saving the PCA Model\n",
    "The PCA model that was previously fit to the HOG features is serialized (saved) to disk using Python's pickle module, allowing it to be loaded and used later without retraining. The file is named 'PCA_model.sav'.\n",
    "Transforming Features Using PCA and Preparing Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09910fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import joblib\n",
    "filename = 'PCA_model.sav'\n",
    "pickle.dump(pca, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b378f",
   "metadata": {},
   "source": [
    "Transforming Features Using PCA and Preparing Final Data\n",
    "The HOG features in x are transformed using the previously trained PCA model to reduce dimensionality. This transformed feature set is converted into a pandas DataFrame, B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B = pca.transform(x)\n",
    "B = pd.DataFrame(B)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451f259",
   "metadata": {},
   "source": [
    "The class labels stored in y are concatenated to B as the final column, ensuring each row in B now consists of the principal components followed by the corresponding class label. This forms the complete dataset for training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinate the Class ID's \n",
    "B=pd.concat([B, pd.DataFrame(y)],axis=1)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc252d78",
   "metadata": {},
   "source": [
    "This final dataset is then saved to a CSV file named 'Final_HOG_Feature.csv' without headers or indexes, making it suitable for loading as raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Final Feature File after applying PCA \n",
    "csv_data1=B.to_csv('Final_HOG_Feature.csv', mode='w',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b3c58",
   "metadata": {},
   "source": [
    "Loading and Preprocessing the Final Dataset\n",
    "The saved CSV file is loaded into a pandas DataFrame, train_data, with no header as the CSV doesn't contain column names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164bbe4",
   "metadata": {},
   "source": [
    "The script checks for any missing values (NaN) in the data. Handling missing values is crucial to prevent errors during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Final_HOG_Feature.csv',header=None)\n",
    "     \n",
    "\n",
    "#Check for NaN under a single DataFrame column\n",
    "train_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddd071",
   "metadata": {},
   "source": [
    "The features (principal components) and labels are separated into X and Y, respectively. X contains all columns except the last one, which is assumed to be the class label column. Y is the last column, containing the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns= 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1971430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y  = train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Applying Classifiers With K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88857e",
   "metadata": {},
   "source": [
    "#### Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1558c58",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1129769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_LR = LogisticRegression(solver= 'liblinear')\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model_LR.fit(X_train,y_train)\n",
    "    pred_values = model_LR.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "joblib.dump(model_LR, \"LG_HOG.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ab2b4",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_RF = RandomForestClassifier()\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model_RF.fit(X_train,y_train)\n",
    "    pred_values = model_RF.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a6163",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_knn = KNeighborsClassifier()\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :].to_numpy(), X.iloc[test_index, :].to_numpy()  # Convert to NumPy array\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    model_knn.fit(X_train, y_train)\n",
    "    pred_values = model_knn.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(pred_values, y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b256b6",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    " #XGBoost\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=None)\n",
    "\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "\n",
    "acc_score = []\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_xgb.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    y_pred_list.extend(y_pred)\n",
    "    y_test_list.extend(y_test)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('Accuracy of each fold - {}'.format(acc_score))\n",
    "print('Average accuracy: {}'.format(avg_acc_score))\n",
    "\n",
    "joblib.dump(model_xgb, \"xgb.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ded3b",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_AB = AdaBoostClassifier()\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model_AB.fit(X_train,y_train)\n",
    "    pred_values = model_AB.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3dc972",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0016947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_SVM = SVC(kernel='linear')\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model_SVM.fit(X_train, y_train)\n",
    "    pred_values = model_SVM.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(pred_values, y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867da9d9",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c924877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_DT = DecisionTreeClassifier()\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model_DT.fit(X_train, y_train)\n",
    "    pred_values = model_DT.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(pred_values, y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819f54b",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d26918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "model_GBM = GradientBoostingClassifier()\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model_GBM.fit(X_train, y_train)\n",
    "    pred_values = model_GBM.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(pred_values, y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f06279",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Results\n",
    "print(\"Pituitary Tumor\")\n",
    "#Reading Image\n",
    "img = cv2.imread(\"pituitary.jpg\")\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "#resize total  image size to 200 x 200\n",
    "resize=(200,200)\n",
    "img1=cv2.resize(img,resize)\n",
    "\n",
    "#Grayscaling the Image\n",
    "gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply median filter with kernel size 3x3\n",
    "median_img = cv2.medianBlur(gray, 3)\n",
    "\n",
    "#Applying HOG Feature Descriptor\n",
    "fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "\n",
    "# Convert the descriptor array into a DataFrame format\n",
    "print(\"descriptor shape \", i, \" : \", fd.shape)\n",
    "\n",
    "#Getting data in a Dataframe\n",
    "df = pd.DataFrame(fd)\n",
    "\n",
    "#Appling Transpose to dataframe to convert rows into column and column into rows\n",
    "df_transposed = df.transpose()\n",
    "df_transposed\n",
    "\n",
    "#Loading the PCA Model\n",
    "pca = joblib.load('PCA_model.sav')\n",
    "\n",
    "#Applying the PCA to extracted Data\n",
    "fd_pca = pca.transform(df_transposed)\n",
    "     \n",
    "\n",
    "# Load the XGBoost model as it provides highest Accuracy\n",
    "loaded_model = joblib.load(\"xgb.sav\")\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "y_pred = loaded_model.predict(fd_pca)\n",
    "\n",
    "print(y_pred)\n",
    "# Glioma = 0 \n",
    "# Meningioma = 1\n",
    "# No tumor = 2\n",
    "# Pituitary = 3\n",
    "\n",
    "if y_pred == 0:\n",
    "    print(\"Given Image is of Glioma Tumor\")\n",
    "elif y_pred == 1:\n",
    "    print(\"Given Image is of Meningioma Tumor\")\n",
    "elif y_pred == 2:\n",
    "    print(\"Given Image is of No Tumor\")\n",
    "else:\n",
    "    print(\"Given Image is of Pituitary Tumor\")\n",
    "    \n",
    "print(\"Glioma Tumor\")\n",
    "img2 = cv2.imread(\"glioma.jpg\")\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "#resize total  image size to 200 x 200\n",
    "resize=(200,200)\n",
    "img3=cv2.resize(img2,resize)\n",
    "\n",
    "#Grayscaling the Image\n",
    "gray = cv2.cvtColor(img3, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply median filter with kernel size 3x3\n",
    "median_img = cv2.medianBlur(gray, 3)\n",
    "\n",
    "#Applying HOG Feature Descriptor\n",
    "fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n",
    "\n",
    "# Convert the descriptor array into a DataFrame format\n",
    "print(\"descriptor shape \", i, \" : \", fd.shape)\n",
    "\n",
    "#Getting data in a Dataframe\n",
    "df = pd.DataFrame(fd)\n",
    "\n",
    "#Appling Transpose to dataframe to convert rows into column and column into rows\n",
    "df_transposed = df.transpose()\n",
    "df_transposed\n",
    "\n",
    "#Loading the PCA Model\n",
    "pca = joblib.load('PCA_model.sav')\n",
    "\n",
    "#Applying the PCA to extracted Data\n",
    "fd_pca = pca.transform(df_transposed)\n",
    "     \n",
    "\n",
    "# Load the XGBoost model as it provides highest Accuracy\n",
    "loaded_model = joblib.load(\"xgb.sav\")\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "y_pred = loaded_model.predict(fd_pca)\n",
    "\n",
    "print(y_pred)\n",
    "# Glioma = 0 \n",
    "# Meningioma = 1\n",
    "# No tumor = 2\n",
    "# Pituitary = 3\n",
    "\n",
    "if y_pred == 0:\n",
    "    print(\"Given Image is of Glioma Tumor\")\n",
    "elif y_pred == 1:\n",
    "    print(\"Given Image is of Meningioma Tumor\")\n",
    "elif y_pred == 2:\n",
    "    print(\"Given Image is of No Tumor\")\n",
    "else:\n",
    "    print(\"Given Image is of Pituitary Tumor\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2b2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9484805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
