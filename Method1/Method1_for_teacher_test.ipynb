{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function load_images_from_folder takes a folder path as input and iterates through each file in the folder. It uses cv2.imread to read each image file. If the image is successfully read (i.e., img is not None), it is appended to the images list. Finally, it returns the list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts an image to grayscale, applies Gaussian blur to reduce noise, and then equalizes the histogram of the image. These steps are typical preprocessing tasks to make feature extraction more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    equalized = cv2.equalizeHist(blurred)\n",
    "    return equalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Features from an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function initializes a SIFT (Scale-Invariant Feature Transform) descriptor, then detects keypoints and computes descriptors for the input image. The keypoints are locations in the image that are considered to have unique features, and descriptors are the features at those keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Features Between Two Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses the FLANN (Fast Library for Approximate Nearest Neighbors) algorithm to match descriptors between two images. knnMatch finds the two nearest neighbors for each descriptor. The function then filters these to retain only good matches based on the distance between the matched descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(descriptors1, descriptors2):\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.7*n.distance]\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying a Tumor in an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes the test image, then iterates through each folder of training images, corresponding to different tumor types. It processes and extracts features from each training image, matches these features with those of the test image, and calculates the average number of matches per folder. The folder (tumor type) with the highest average match count is considered the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tumor(test_image, train_folders):\n",
    "    preprocessed_test_image = preprocess_image(test_image)\n",
    "    print(train_folders)\n",
    "    _, test_descriptors = extract_features(preprocessed_test_image)\n",
    "    max_matches = 0\n",
    "    tumor_type = None\n",
    "\n",
    "    for folder in train_folders:\n",
    "        total_matches = 0\n",
    "        train_images = load_images_from_folder(folder)\n",
    "        for train_image in train_images:\n",
    "            preprocessed_train_image = preprocess_image(train_image)\n",
    "            _, train_descriptors = extract_features(preprocessed_train_image)\n",
    "            matches = feature_matching(test_descriptors, train_descriptors)\n",
    "            total_matches += len(matches)\n",
    "\n",
    "        avg_matches = total_matches / len(train_images)\n",
    "        print(f\"Average matches for {os.path.basename(folder)}: {avg_matches}\")\n",
    "\n",
    "        if avg_matches > max_matches:\n",
    "            max_matches = avg_matches\n",
    "            tumor_type = os.path.basename(folder)\n",
    "\n",
    "    return tumor_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Paths and Classifying a Test Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part sets up the file paths for the training folders and the test image, reads the test image using OpenCV, and calls classify_tumor with the test image and the training folders. It then prints out the classification result, indicating the type of tumor identified in the test image.\n",
    "\n",
    "Since it takes too long to test all the test data, it was tested for only 1 image so that you can test it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\bltkb\\\\Desktop\\\\spring 24\\\\Bil 468 Proje\\\\Method1\\\\Training\\\\meningioma_tumor', 'C:\\\\Users\\\\bltkb\\\\Desktop\\\\spring 24\\\\Bil 468 Proje\\\\Method1\\\\Training\\\\glioma_tumor', 'C:\\\\Users\\\\bltkb\\\\Desktop\\\\spring 24\\\\Bil 468 Proje\\\\Method1\\\\Training\\\\no_tumor', 'C:\\\\Users\\\\bltkb\\\\Desktop\\\\spring 24\\\\Bil 468 Proje\\\\Method1\\\\Training\\\\pituitary_tumor']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average matches for meningioma_tumor: 4.54683698296837\n",
      "Average matches for glioma_tumor: 4.953389830508475\n"
     ]
    }
   ],
   "source": [
    "train_base_folder = r'C:\\Users\\bltkb\\Desktop\\spring 24\\Bil 468 Proje\\Method1\\dataset1\\Training'\n",
    "train_folders = [os.path.join(train_base_folder, f) for f in ['meningioma_tumor','glioma_tumor', 'no_tumor', 'pituitary_tumor']]\n",
    "\n",
    "test_image_path = r'C:\\Users\\bltkb\\Desktop\\spring 24\\Bil 468 Proje\\Method1\\dataset1\\Testing\\meningioma_tumor\\image.jpg'\n",
    "\n",
    "test_image = cv2.imread(test_image_path)\n",
    "tumor_type = classify_tumor(test_image, train_folders)\n",
    "print(f\"The test image is classified as: {tumor_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
